{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20388e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\greeshma.ganta\\Anaconda3\\lib\\site-packages\\flask\\json\\__init__.py:31: DeprecationWarning: Importing 'itsdangerous.json' is deprecated and will be removed in ItsDangerous 2.1. Use Python's 'json' module instead.\n",
      "  _slash_escape = \"\\\\/\" not in _json.dumps(\"/\")\n",
      "c:\\Users\\greeshma.ganta\\Anaconda3\\lib\\site-packages\\flask\\json\\__init__.py:61: DeprecationWarning: Importing 'itsdangerous.json' is deprecated and will be removed in ItsDangerous 2.1. Use Python's 'json' module instead.\n",
      "  class JSONEncoder(_json.JSONEncoder):\n",
      "c:\\Users\\greeshma.ganta\\Anaconda3\\lib\\site-packages\\flask\\json\\__init__.py:103: DeprecationWarning: Importing 'itsdangerous.json' is deprecated and will be removed in ItsDangerous 2.1. Use Python's 'json' module instead.\n",
      "  class JSONDecoder(_json.JSONDecoder):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\greeshma.ganta\\\\Anaconda3\\\\lib\\\\site-packages\\\\pyspark'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import sentry_sdk\n",
    "\n",
    "sentry_sdk.init(\n",
    "    dsn=\"https://035a834189a646d6994ad814216c5afa@o4504756322304000.ingest.sentry.io/4504790342434816\",\n",
    "\n",
    "    # Set traces_sample_rate to 1.0 to capture 100%\n",
    "    # of transactions for performance monitoring.\n",
    "    # We recommend adjusting this value in production.\n",
    "    traces_sample_rate=1.0\n",
    ")\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b3e18e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\GREESH~1.GAN\\AppData\\Local\\Temp/ipykernel_22408/2488486328.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mail_notify\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\greeshma.ganta\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m                     \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m                     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\greeshma.ganta\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\greeshma.ganta\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[0;32m    193\u001b[0m             )\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             self._do_init(\n",
      "\u001b[1;32mc:\\Users\\greeshma.ganta\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\greeshma.ganta\\Anaconda3\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[1;31m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m                 \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"mail_notify\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6bc3250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: great_expectations===0.14.12 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (0.14.12)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from great_expectations===0.14.12) (3.10.0.2)\n",
      "Requirement already satisfied: colorama>=0.4.3 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from great_expectations===0.14.12) (0.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from great_expectations===0.14.12) (2.8.2)\n",
      "Requirement already satisfied: jinja2<3.1.0,>=2.10 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from great_expectations===0.14.12) (2.11.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from great_expectations===0.14.12) (1.26.14)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from great_expectations===0.14.12) (2.4.7)\n",
      "Requirement already satisfied: tqdm>=4.59.0 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from great_expectations===0.14.12) (4.62.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from great_expectations===0.14.12) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.14.1 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from great_expectations===0.14.12) (1.20.3)\n",
      "Requirement already satisfied: tzlocal>=1.2 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from great_expectations===0.14.12) (4.2)\n",
      "Requirement already satisfied: ruamel.yaml<0.17.18,>=0.16 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from great_expectations===0.14.12) (0.17.17)\n",
      "Requirement already satisfied: altair<5,>=4.0.0 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from great_expectations===0.14.12) (4.2.2)\n",
      "Requirement already satisfied: cryptography>=3.2 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from great_expectations===0.14.12) (3.4.8)\n",
      "Requirement already satisfied: Click>=7.1.2 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from great_expectations===0.14.12) (8.0.3)\n",
      "Requirement already satisfied: scipy>=0.19.0 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from great_expectations===0.14.12) (1.7.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.7.0 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from great_expectations===0.14.12) (4.8.1)\n",
      "Requirement already satisfied: jsonpatch>=1.22 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from great_expectations===0.14.12) (1.32)\n",
      "Requirement already satisfied: pytz>=2021.3 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from great_expectations===0.14.12) (2021.3)\n",
      "Requirement already satisfied: ipywidgets>=7.5.1 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from great_expectations===0.14.12) (7.6.5)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from great_expectations===0.14.12) (2.26.0)\n",
      "Requirement already satisfied: pandas>=0.23.0 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from great_expectations===0.14.12) (1.3.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from great_expectations===0.14.12) (21.0)\n",
      "Requirement already satisfied: nbformat>=5.0 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from great_expectations===0.14.12) (5.1.3)\n",
      "Requirement already satisfied: jsonschema>=2.5.1 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from great_expectations===0.14.12) (3.2.0)\n",
      "Requirement already satisfied: mistune<2.0.0,>=0.8.4 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from great_expectations===0.14.12) (0.8.4)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from altair<5,>=4.0.0->great_expectations===0.14.12) (0.3)\n",
      "Requirement already satisfied: toolz in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from altair<5,>=4.0.0->great_expectations===0.14.12) (0.11.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from cryptography>=3.2->great_expectations===0.14.12) (1.14.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=3.2->great_expectations===0.14.12) (2.20)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from importlib-metadata>=1.7.0->great_expectations===0.14.12) (3.6.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from ipywidgets>=7.5.1->great_expectations===0.14.12) (6.4.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from ipywidgets>=7.5.1->great_expectations===0.14.12) (5.1.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from ipywidgets>=7.5.1->great_expectations===0.14.12) (1.0.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from ipywidgets>=7.5.1->great_expectations===0.14.12) (7.29.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from ipywidgets>=7.5.1->great_expectations===0.14.12) (3.5.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from ipywidgets>=7.5.1->great_expectations===0.14.12) (0.2.0)\n",
      "Requirement already satisfied: jupyter-client<8.0 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->great_expectations===0.14.12) (6.1.12)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.1.2)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->great_expectations===0.14.12) (6.1)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->great_expectations===0.14.12) (1.4.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (5.1.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (58.0.4)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.7.5)\n",
      "Requirement already satisfied: pygments in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (2.10.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (3.0.20)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.18.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from jinja2<3.1.0,>=2.10->great_expectations===0.14.12) (1.1.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from jsonpatch>=1.22->great_expectations===0.14.12) (2.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from jsonschema>=2.5.1->great_expectations===0.14.12) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from jsonschema>=2.5.1->great_expectations===0.14.12) (0.18.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from jsonschema>=2.5.1->great_expectations===0.14.12) (1.16.0)\n",
      "Requirement already satisfied: pyzmq>=13 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets>=7.5.1->great_expectations===0.14.12) (22.2.1)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets>=7.5.1->great_expectations===0.14.12) (4.8.1)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from jupyter-core>=4.6.0->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets>=7.5.1->great_expectations===0.14.12) (228)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.2.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from requests>=2.20->great_expectations===0.14.12) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from requests>=2.20->great_expectations===0.14.12) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from requests>=2.20->great_expectations===0.14.12) (2021.10.8)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.1.2 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from ruamel.yaml<0.17.18,>=0.16->great_expectations===0.14.12) (0.2.7)\n",
      "Requirement already satisfied: tzdata in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from tzlocal>=1.2->great_expectations===0.14.12) (2022.7)\n",
      "Requirement already satisfied: pytz-deprecation-shim in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from tzlocal>=1.2->great_expectations===0.14.12) (0.1.0.post0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (6.4.5)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.11.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.9.4)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (6.1.0)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (1.8.0)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (20.1.0)\n",
      "Requirement already satisfied: pywinpty>=0.5 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.5.7)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.5.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (1.4.3)\n",
      "Requirement already satisfied: bleach in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (4.0.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.1.2)\n",
      "Requirement already satisfied: testpath in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.5.0)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (1.5.1)\n",
      "Requirement already satisfied: async-generator in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (1.10)\n",
      "Requirement already satisfied: webencodings in c:\\users\\greeshma.ganta\\anaconda3\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install great_expectations===0.14.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac404679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "#sys.path.insert(1,\"C:\\\\Users\\\\abhinav.anand\\\\Desktop\\\\dq_check\")\n",
    "from great_expectations.data_context.types.base import DatasourceConfig\n",
    "from great_expectations.core.batch import RuntimeBatchRequest\n",
    "from great_expectations.data_context.types.base import DataContextConfig, DatasourceConfig, FilesystemStoreBackendDefaults\n",
    "from great_expectations.data_context import BaseDataContext\n",
    "from great_expectations.exceptions import DataContextError\n",
    "from great_expectations.core.expectation_configuration import ExpectationConfiguration\n",
    "from great_expectations.render.renderer import ValidationResultsPageRenderer\n",
    "from ruamel import yaml\n",
    "from multiprocessing.pool import Pool, ThreadPool\n",
    "import multiprocessing\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import concat_ws\n",
    "from pyspark.sql.window import Window\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import datetime\n",
    "import configparser\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abb4458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run \"C:/Users/abhinav.anand/Desktop/dq_check/custom_expectations/expect_column_values_to_contain_valid_email.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31ccaca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run \"C:/Users/abhinav.anand/Desktop/dq_check/custom_expectations/expect_column_values_to_not_contain_special_characters.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25395d45",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (Temp/ipykernel_22408/2807605553.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\GREESH~1.GAN\\AppData\\Local\\Temp/ipykernel_22408/2807605553.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    CONFIG.read(\"C:\\Users\\greeshma.ganta\\Desktop\\dqc\\dataquality-\\dqc\\config.ini\")\u001b[0m\n\u001b[1;37m                                                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "CONFIG= configparser.ConfigParser()\n",
    "CONFIG.read(\"C:\\Users\\greeshma.ganta\\Desktop\\dqc\\dataquality-\\dqc\\config.ini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45b43aab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CONFIG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\GREESH~1.GAN\\AppData\\Local\\Temp/ipykernel_22408/2082435993.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"DQ_CHECK_CONFIG\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"RULES_REPO\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'CONFIG' is not defined"
     ]
    }
   ],
   "source": [
    "print(CONFIG[\"DQ_CHECK_CONFIG\"][\"RULES_REPO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcf6e71c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (Temp/ipykernel_22408/3029777670.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\GREESH~1.GAN\\AppData\\Local\\Temp/ipykernel_22408/3029777670.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    log_file='C:\\Users\\greeshma.ganta\\Desktop\\dqc\\logs'+'dq_ge_'+str(file_date)+'.log'\u001b[0m\n\u001b[1;37m                                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "file_date = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "log_file='C:\\Users\\greeshma.ganta\\Desktop\\dqc\\logs'+'dq_ge_'+str(file_date)+'.log'\n",
    "format_log = logging.Formatter('%(asctime)s  %(levelname)-8s [%(name)s:%(lineno)d]  %(message)s')\n",
    "\n",
    "logger = logging.getLogger('dq_ge')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "fh = logging.FileHandler(log_file, mode='a')\n",
    "\n",
    "sh = logging.StreamHandler()\n",
    "sh.setLevel(logging.DEBUG)\n",
    "\n",
    "fh.setFormatter(format_log)\n",
    "sh.setFormatter(format_log)\n",
    "if (logger.hasHandlers()):\n",
    "  logger.handlers.clear()\n",
    "logger.addHandler(sh)\n",
    "logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d97eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeDQFrameWork:\n",
    "  \n",
    "    #Create_data_source_config\n",
    "    def _data_source_config(self,connector, batch_id, engine):\n",
    "      data_source_config = DatasourceConfig(\n",
    "          class_name=\"Datasource\",\n",
    "          execution_engine={\"class_name\": engine },\n",
    "          data_connectors={\n",
    "            connector : {\n",
    "              \"module_name\": \"great_expectations.datasource.data_connector\",\n",
    "              \"class_name\": \"RuntimeDataConnector\",\n",
    "              \"batch_identifiers\": [str(batch_id)] }\n",
    "          }\n",
    "        )\n",
    "      logger.info(f'{connector} - Datasource Genereted')\n",
    "      return data_source_config\n",
    "\n",
    "    \n",
    "    #Create_project_config\n",
    "    def _create_project_config(self, spark_datasource_name, spark_data_source_config,pandas_datasource_name, pandas_data_source_config):\n",
    "        project_config = DataContextConfig( datasources={\n",
    "            spark_datasource_name : spark_data_source_config, pandas_datasource_name:pandas_data_source_config}, \n",
    "                                           stores={ \"expectations_store\": {\n",
    "                  \"class_name\": \"ExpectationsStore\",\n",
    "                  \"store_backend\": { \"class_name\": \"TupleFilesystemStoreBackend\",\n",
    "                  \"base_directory\": CONFIG[\"DQ_CHECK_CONFIG\"][\"EXPECTATION_PATH\"]}, },\n",
    "                  \"validations_store\": {\n",
    "                      \"class_name\": \"ValidationsStore\",\n",
    "                      \"store_backend\": { \"class_name\": \"TupleFilesystemStoreBackend\",\n",
    "                      \"base_directory\": CONFIG[\"DQ_CHECK_CONFIG\"][\"VALIDATION_PATH\"],\n",
    "                        },},\n",
    "                   \"evaluation_parameter_store\": { \"class_name\": \"EvaluationParameterStore\"},\n",
    "                                                    \n",
    "              }, \n",
    "        validation_operators={ \"action_list_operator\": {\n",
    "                \"class_name\": \"ActionListValidationOperator\",\n",
    "                \"action_list\": [{\n",
    "                        \"name\": \"store_validation_result\",\n",
    "                        \"action\": {\"class_name\": \"StoreValidationResultAction\"},\n",
    "                    },\n",
    "                   {\n",
    "                         \"name\": \"store_evaluation_params\",\n",
    "                         \"action\": {\"class_name\": \"StoreEvaluationParametersAction\"},\n",
    "                     },\n",
    "                     {\n",
    "                        \"name\": \"update_data_docs\",\n",
    "                        \"action\": {\"class_name\": \"UpdateDataDocsAction\"},\n",
    "                    }\n",
    "#                     {\n",
    "#                       \"name\": \"send_email_on_validation_result\",\n",
    "#                       \"action\": {\n",
    "#                         \"class_name\": \"EmailAction\",\n",
    "#                         \"notify_on\": \"all\",\n",
    "#                         \"use_tls\": True,\n",
    "#                         \"use_ssl\": False,\n",
    "#                         \"renderer\": {\n",
    "#                           \"module_name\": \"great_expectations.render.renderer.email_renderer\",\n",
    "#                           \"class_name\": \"EmailRenderer\"\n",
    "#                         },\n",
    "#                         \"smtp_address\": \"smtp-mail.outlook.com\",\n",
    "#                         \"smtp_port\": 587,\n",
    "#                         \"sender_login\": \"abhinavv.anand@outlook.com\",\n",
    "#                         \"sender_password\": \"****\",\n",
    "#                         \"sender_alias\": \"abhinavv.anand@outlook.com\",\n",
    "#                         \"receiver_emails\": \"abhinavanandpnbe@gmail.com\",\n",
    "#                       }\n",
    "#                     }\n",
    "                     \n",
    "                ]} },\n",
    "           data_docs_sites = {\n",
    "                    \"local_site\": {\n",
    "                    \"class_name\": \"SiteBuilder\",\n",
    "                    \"store_backend\": { \"class_name\": \"TupleFilesystemStoreBackend\",\n",
    "                                     \"base_directory\": CONFIG[\"DQ_CHECK_CONFIG\"][\"DATA_DOCS_PATH\"]},\n",
    "                    \"site_index_builder\": {\"class_name\": \"DefaultSiteIndexBuilder\",\n",
    "                                     \"show_cta_footer\": True, },} },\n",
    "                                           \n",
    "             store_backend_defaults=FilesystemStoreBackendDefaults(root_directory=CONFIG[\"DQ_CHECK_CONFIG\"][\"VALIDATION_PATH\"])\n",
    "            )\n",
    "        logger.info('Project Configuration Created')\n",
    "        return project_config\n",
    "\n",
    "    #create_batch_request\n",
    "    def _create_batch_request(self, datasource_name, data_connector_name, data_asset_name,data_batch,batch_id, run_id):\n",
    "      batch_request = RuntimeBatchRequest(\n",
    "        datasource_name= datasource_name,\n",
    "        data_connector_name= data_connector_name,\n",
    "        data_asset_name= data_asset_name,\n",
    "        runtime_parameters= { \"batch_data\": data_batch },\n",
    "        batch_identifiers= {batch_id: run_id})\n",
    "      logger.info(f'{datasource_name} - Batchrequest Created !!!')\n",
    "      return batch_request\n",
    "\n",
    "    #add_expectations_to_the_suite\n",
    "    def _add_expectations_to_suite(self,data,suite):\n",
    "      data_meta = None\n",
    "      for row in data.collect():\n",
    "        if row['source_type'] == 'csv': \n",
    "          data_meta = {\"file_name\": row[\"source\"]}\n",
    "        kwargs = json.loads('{'+str(row[\"parameter\"])+'}')\n",
    "        expectation_configuration = ExpectationConfiguration(expectation_type = row[\"validation_name\"], kwargs =kwargs, meta = data_meta)\n",
    "        suite.add_expectation(expectation_configuration=expectation_configuration,match_type=\"domain\")\n",
    "        logger.info(f'Expectation - {row[\"validation_name\"]} - Created for:  {row[\"source\"]}')\n",
    "      return \n",
    "    \n",
    "    #create_validadtor\n",
    "    def _create_validator(self, batch_request,context, suite):\n",
    "      data_validator = context.get_validator(batch_request=batch_request,expectation_suite=suite)\n",
    "      logger.info(\"Validator Created\")\n",
    "      data_validator.save_expectation_suite(discard_failed_expectations=False)\n",
    "      return data_validator\n",
    "\n",
    "    #read_dataframe\n",
    "    def _read_data_frame(self, path):\n",
    "      data_frame = spark.read.format(\"csv\").options(inferSchema = \"true\" ,multiline = \"true\",header = \"true\", sep= \",\",quote=\"\\\"\",escape=\"\\\"\").load(path)\n",
    "      return data_frame\n",
    "#     def _read_data_json(self, path):\n",
    "#     # Opening JSON file\n",
    "#       f = open(path)\n",
    "#       dataf = json.load(f)\n",
    "#       return dataf\n",
    "    \n",
    "    \n",
    "    def _get_failed_expectations(self,resultss,df):\n",
    "      failed_expectations = list()\n",
    "      if isinstance(resultss, list):\n",
    "          failed_expectations = list(__builtins__.filter(lambda x: x['success']== False, resultss))\n",
    "      return self._bad_records_count(failed_expectations,df)\n",
    "        \n",
    "\n",
    "    def _bad_records_count(self,failed_expectations,df):\n",
    "        bad_records_df = df.filter(\"1 == 2\")##check\n",
    "        good_records_df_count = None\n",
    "        for f in failed_expectations:\n",
    "            column_name = f['expectation_config']['kwargs'].get('column')\n",
    "            unexpected_list = f['result'].get('unexpected_list')\n",
    "\n",
    "            if f['expectation_config'].get('expectation_type') == 'expect_column_values_to_not_be_null':\n",
    "                data_df = df.filter(col(column_name).isNull())\n",
    "\n",
    "\n",
    "            if column_name is not None:\n",
    "                if unexpected_list is not None and len(unexpected_list) > 0:   \n",
    "                    temp_df = df.filter(col(column_name).isin(set(unexpected_list)))\n",
    "                    if f['expectation_config'].get('expectation_type') == 'expect_column_values_to_not_be_null':\n",
    "                        temp_df = data_df\n",
    "                        \n",
    "                    #data_df.show() \n",
    "                    bad_records_df = bad_records_df.union(temp_df)\n",
    "                    good_records_df = df.subtract(bad_records_df)\n",
    "                    try:\n",
    "                        if good_records_df.count() > 0:\n",
    "#                            good_records_df_count = good_records_df.dropDuplicates().count()\n",
    "                            good_records_df_count = good_records_df.dropDuplicates().count()\n",
    "                            #logger.info(f'Good Records Count: {good_records_df_count}')\n",
    "                    except Exception as e:\n",
    "                        logger.error(f'{e} Exception occured in Getting good records')\n",
    "#         bad_records = bad_records_df.coalesce(1).write.option(\"header\", \"true\").mode(\"append\").csv(\"C:/Users/abhinav.anand/Desktop/dq_check/\"+\"bad_rec\"+'_'+str(file_date)+\".csv\")\n",
    "#         good_records = good_records_df.coalesce(1).write.option(\"header\", \"true\").mode(\"append\").csv(\"C:/Users/abhinav.anand/Desktop/dq_check/\"+\"good_rec\"+'_'+str(file_date)+\".csv\")            \n",
    "        logger.info(f'Good Records Count: {good_records_df_count}')         \n",
    "        if bad_records_df.count()   > 0:\n",
    "            bad_records_df_count = bad_records_df.count()\n",
    "#            bad_records_df_count = bad_records_df.dropDuplicates().count()\n",
    "            \n",
    "            return bad_records_df_count\n",
    "    \n",
    "          \n",
    "    def _get_bad_records(self,validation_ids, validations_results_dict, data_df):\n",
    "              results_dict = None\n",
    "              bad_rec_count = None\n",
    "              for ind,val in enumerate(validation_ids): \n",
    "                logger.info(f'Validation Id:{val}')\n",
    "                if ind == 0:\n",
    "                    results_dict = validations_results_dict['run_results'][str(val)]['validation_result']['results']\n",
    "                if ind >= 1:\n",
    "                    for i in validations_results_dict['run_results'][str(val)]['validation_result']['results']:\n",
    "                        results_dict.append(i)\n",
    "              try:\n",
    "                if results_dict != None:\n",
    "                  bad_rec_count = self._get_failed_expectations(resultss=results_dict, df=data_df)\n",
    "                  logger.info(f'Bad Records Count: {bad_rec_count}')\n",
    "              except Exception as e:\n",
    "                logger.error(f'{e} Exception occured in Getting bad records')\n",
    "              return bad_rec_count\n",
    "\n",
    "    \n",
    "    #Mandatory_optional_field_checking\n",
    "    #Mandatory_optional_field_checking\n",
    "    def _mandatory_option_check(self, joined_table):\n",
    "        df_list=[]\n",
    "        combinedDF = joined_table.select( col(\"id\"),col(\"parameter\"), col(\"parameter_name\"),col(\"parameter_required\"))\n",
    "        df1 = combinedDF.withColumn('new', concat_ws(',',split(regexp_replace(regexp_replace(col('parameter'),' ',''),'\"',\"'\"), ':')))\n",
    "        quoted_page_name = concat(lit(\"'\"), col(\"parameter_name\"), lit(\"'\"))\n",
    "        res = df1.withColumn(\"IDmatch\", when(col(\"new\").contains(quoted_page_name), True).otherwise(False))\n",
    "        res.withColumn(\"size\",when(size(split(regexp_replace(col(\"parameter\"),':([^,]*)', \"~#~$1\"),'~#~'))==1,1).otherwise(size(split(regexp_replace(col(\"parameter\"),':([^,]*)', \"~#~$1\"),'~#~'))-1)).registerTempTable('res')\n",
    "        if (res.filter((res.IDmatch == \"False\") & (res.parameter_required == 'MANDATORY')).count() >0 ):\n",
    "            df_list.append(res.select(col(\"id\"),lit('MANDATORY').alias('error')).filter((res.IDmatch == \"False\") & (res.parameter_required == 'MANDATORY') ))\n",
    "        if spark.sql(\"select id,IDmatch,size from res group by id,IDmatch,size having count(*) != size and IDmatch == 'true'\").count() > 0:\n",
    "            df_list.append(spark.sql(\"select id ,'OPTIONAL' from res group by id,IDmatch,size having count(*) != size and IDmatch == 'true'\"))\n",
    "        return df_list\n",
    "    \n",
    "    def _check_column(self,source_table,combined_table):\n",
    "      list_1 = []\n",
    "      combined_table = combined_table.withColumn(\"table_columns\",lit(','.join(source_table.columns)))\n",
    "      if combined_table.filter(col(\"parameter\").contains('\"column\"')).count() >0 :\n",
    "          combined_table_filterd_column = combined_table.filter(col(\"parameter\").contains('\"column\"'))\n",
    "          combined_table_filterd_column = combined_table_filterd_column.withColumn('new', regexp_replace(regexp_replace(split(split('parameter',',')[0],':')[1],'\"',''),' ',''))\n",
    "          res = combined_table_filterd_column.withColumn(\"column_match\", when(col(\"table_columns\").contains(col(\"new\")), True).otherwise(False))      \n",
    "          if res.filter(col(\"column_match\") == False).count() >0: list_1.append(res.filter(col(\"column_match\") == False))\n",
    "      if combined_table.filter(col(\"parameter\").contains('\"column_list\"')).count() >0 :\n",
    "          combined_table_filterd_column_list = combined_table.filter(col(\"parameter\").contains('\"column_list\"'))\n",
    "          combined_table_filterd_column_list = combined_table_filterd_column_list.withColumn('new',explode(split(regexp_replace(regexp_replace(regexp_replace(regexp_replace(split(col('parameter'),':')[1],'\"',\"\"),'[',''),']',''),' ',''),',')))\n",
    "          res_column_list = combined_table_filterd_column_list.withColumn(\"column_match\", when(col(\"table_columns\").contains(col(\"new\")), True).otherwise(False))\n",
    "          if res_column_list.filter(col(\"column_match\") == False).count() >0: list_1.append(res_column_list.filter(col(\"column_match\") == False))\n",
    "      if combined_table.filter(col(\"parameter\").contains('\"column_A\"')).count() >0 :\n",
    "          combined_table_filterd_column_mul = combined_table.filter(col(\"parameter\").contains('\"column_A\"') | col(\"parameter\").contains('\"column_B\"'))\n",
    "          combined_table_filterd_column_mul = combined_table_filterd_column_mul.withColumn('new',explode(split(regexp_replace(regexp_replace(concat(split(split('parameter',',')[0],':')[1],lit(\"~#~\"),split(split('parameter',',')[1],\":\")[1]),'\"',''),' ',''),'~#~')))\n",
    "          res_column_mul = combined_table_filterd_column_mul.withColumn(\"column_match\", when(col(\"table_columns\").contains(col(\"new\")), True).otherwise(False))\n",
    "          if res_column_mul.filter(col(\"column_match\") == False).count() >0: list_1.append(res_column_mul.filter(col(\"column_match\") == False))\n",
    "      return list_1\n",
    "    \n",
    "    \n",
    "    \n",
    "    #read_rules_repository\n",
    "    def _get_rules_repo(self, file_name):\n",
    "      logger.info(\"Read Rules Repository Files - Started\")\n",
    "      dq_validation = self._read_data_frame(path=CONFIG[\"DQ_CHECK_CONFIG\"][\"DQ_VALIDATION\"])\n",
    "      source_validation = self._read_data_frame(path=CONFIG[\"DQ_CHECK_CONFIG\"][\"SOURCE_VALIDATION\"])\n",
    "      source_meta = self._read_data_frame(path=CONFIG[\"DQ_CHECK_CONFIG\"][\"SOURCE_META\"])\n",
    "      joined_table = self._read_data_frame(path=CONFIG[\"DQ_CHECK_CONFIG\"][\"RULES_REPO\"])\n",
    "      validation_parameter = self._read_data_frame(path=CONFIG[\"DQ_CHECK_CONFIG\"][\"VALIDATION_LIBRARY_PARAMETER\"])\n",
    "      joined_table = source_validation.alias(\"sv\").join(dq_validation.alias('dv'), source_validation.dqvalidation_id == dq_validation.id,\"inner\").join(\n",
    "          source_meta.alias('sm'),source_validation.source_meta_id == source_meta.id,\"left\")\n",
    "      mandatory_optional_check = source_validation.alias(\"sv\").join(\n",
    "         validation_parameter.alias('vp'),validation_parameter.dqvalidation_id == source_validation.dqvalidation_id,\"left\")\n",
    "      man_df = self._mandatory_option_check(mandatory_optional_check)\n",
    "      if len(man_df) == 0: \n",
    "          logger.info('Mandatory Optional Check Completed')\n",
    "      else: \n",
    "        logger.info('Issue in Mandatory Optional Check...')\n",
    "        raise Exception(\"Parameter missing or wrong entry in the rule repo\")\n",
    "      add_db_table = joined_table.withColumn(\"source\", concat_ws(\"\",col('file_name')))\n",
    "      db_tables = add_db_table.filter(add_db_table.source == str(f'{file_name}'))\n",
    "\n",
    "      return db_tables\n",
    "    \n",
    "\n",
    "    def _pre_process(self,df,validation_data):\n",
    "      config_data = {}\n",
    "      list_return=[]\n",
    "      for (each_key, each_val) in CONFIG.items(\"PRE_PROCESS_CONFIG\"):\n",
    "        config_data[each_key] = each_val\n",
    "        pre_process_list = validation_data.select('parameter','validation_name').collect()\n",
    "        for pre_process_item in pre_process_list:\n",
    "          if pre_process_item['validation_name'] in config_data.keys() :\n",
    "            column_name = str(pre_process_item['parameter'].split(',')[0].split(':')[1]).replace('\"','').replace(' ','')\n",
    "            if df.schema[column_name].dataType != 'DoubleType' and config_data[pre_process_item['validation_name']]=='DoubleType':\n",
    "              df=df.withColumn(column_name, df[column_name].cast(DoubleType()))\n",
    "            if df.schema[column_name].dataType != 'IntegerType' and config_data[pre_process_item['validation_name']]=='IntegerType':\n",
    "              df=df.withColumn(column_name, df[column_name].cast(IntegerType()))\n",
    "      list_return.append(df)\n",
    "      if validation_data.filter(col(\"validation_name\")=='expect_column_values_to_change_between').count()>0:\n",
    "        data_list=validation_data.filter(col(\"validation_name\")=='expect_column_values_to_change_between').collect()\n",
    "        expectaion=data_list[0]\n",
    "        column_name = (((expectaion['parameter'].split(',')[0]).split(':')[1]).replace('\"','')).replace(' ','')\n",
    "        from_val = (((expectaion['parameter'].split(',')[1]).split(':')[1]).replace('\"','')).replace(' ','')\n",
    "        to_val = (((expectaion['parameter'].split(',')[2]).split(':')[1]).replace('\"','')).replace(' ','')\n",
    "        my_window = Window.partitionBy().orderBy(column_name)\n",
    "        df = df.withColumn(column_name+\"_diff\", lead(col(column_name)).over(my_window))\n",
    "        df=df.withColumn(column_name+\"_diff\",col(column_name+\"_diff\")-col(column_name))\n",
    "        param = '\"column\" : \"'+column_name+'_diff\" , \"min_value\" : '+str(from_val)+' , \"max_value\" : '+str(to_val)\n",
    "        validation_data=validation_data.withColumn('parameter', regexp_replace(col('parameter'),str(expectaion['parameter']), param))\n",
    "        validation_data=validation_data.withColumn('validation_name', regexp_replace(col('validation_name'),str(expectaion['validation_name']), 'expect_column_values_to_be_between'))\n",
    "        validation_data=validation_data.withColumn('type',when(col(\"validation_name\") == lit('expect_column_values_to_be_between'),regexp_replace(col(\"type\"),'pandas','spark')).otherwise(col(\"type\")))\n",
    "        list_return[0]=df\n",
    "        list_return.append(validation_data)\n",
    "      return list_return\n",
    "    \n",
    "    def actual_process(self,file_name):\n",
    "      logger.info(\"Framework has been Started\")\n",
    "      logger.info(\"actual process has been triggered\")\n",
    "        \n",
    "      \"\"\"\n",
    "      step 1: Read rules repository\n",
    "      step 2: initialize dataContext with Project configuration\n",
    "      step 3: Batch creation\n",
    "      step 4: Create expectation Suite\n",
    "      step 5: Add expectations to the Expectation suite\n",
    "      step 6: Creation of Validator\n",
    "      step 7: Validate data\n",
    "      \"\"\"\n",
    "      \n",
    "      rules_repo = self._get_rules_repo(file_name)\n",
    "      logger.info(f\"Rules Repository Dataframe Generated\")\n",
    "      if rules_repo.rdd.isEmpty() == False:\n",
    "          logger.info(\"Rules Repository Dataframe Have the data\")\n",
    "          spark_datasource_config = self._data_source_config(connector= \"SparkDataconnector\",\n",
    "                                                             batch_id = \"spark_source\",engine = \"SparkDFExecutionEngine\")\n",
    "          pandas_datasource_config = self._data_source_config(connector= \"PandasDataconnector\",\n",
    "                                                              batch_id = \"pandas_source\",engine = \"PandasExecutionEngine\") \n",
    "\n",
    "          project_config=self._create_project_config(spark_datasource_name=\"spark_source\", \n",
    "                                                     spark_data_source_config=spark_datasource_config,\n",
    "                                                     pandas_datasource_name= \"pandas_source\", pandas_data_source_config = pandas_datasource_config)\n",
    "          context = BaseDataContext(project_config=project_config)\n",
    "          logger.info(\"Context Created with Project Configuration\")\n",
    "          rules_source = rules_repo.dropDuplicates(['source']).collect()\n",
    "          run_id = f'{datetime.datetime.now()}'\n",
    "          try:\n",
    "            if rules_source[0]['source_type'] == 'table':\n",
    "              logger.info(\"Source type - Table\")\n",
    "              data_df = spark.table(rules_source[0]['source'])\n",
    "              check_column = self._check_column(data_df,rules_repo)\n",
    "              if len(check_column) ==0: logger.info(\"Column Check Completed\")                         \n",
    "              else: raise Exception(\"Column not present in the Source Table\")\n",
    "              logger.info(\"PreProcessing Started\")\n",
    "              data_list = self._pre_process(data_df,rules_repo)\n",
    "              if len(data_list)==2 : \n",
    "                data_df = data_list[0]\n",
    "                rules_repo = data_list[1]\n",
    "              elif len(data_list)==1 :\n",
    "                data_df = data_list[0]\n",
    "              logger.info(\"PreProcessing Completed\")\n",
    "            elif rules_source[0]['source_type'] == 'csv':\n",
    "              data_df = spark.read.format(rules_source[0]['source_type']).options(inferSchema = \"true\",header = \"true\", sep= \",\",quote=\"\\\"\",\n",
    "                                                                                  escape=\"\\\"\").load(rules_source[0]['source_path'])\n",
    "          except Exception as e:\n",
    "            logger.error(f\"{e} - Exception Occured in Reading the Source data\")\n",
    "            raise Exception(\"Exit\")\n",
    "          data_type = [i['type'] for i in rules_repo.select(col('type')).collect()]\n",
    "          spark_data = rules_repo.filter(col('type') == 'spark')\n",
    "          pandas_data = rules_repo.filter(col('type') == 'pandas')\n",
    "          validator = []\n",
    "          try:\n",
    "            if 'spark' in data_type: \n",
    "              batch_request = self._create_batch_request(datasource_name=\"spark_source\",\n",
    "                                                         data_connector_name=\"SparkDataconnector\", \n",
    "                                                         data_asset_name=rules_source[0]['source'],\n",
    "                                                         data_batch=data_df,batch_id=\"spark_source\", \n",
    "                                                         run_id= run_id)\n",
    "              suite = context.create_expectation_suite(rules_source[0]['source'], overwrite_existing=True)\n",
    "              logger.info(f'{rules_source[0][\"source\"]} - Spark Expectation Suite Created !!!')\n",
    "              self._add_expectations_to_suite(data=spark_data,suite=suite)\n",
    "              validator.append(self._create_validator(batch_request=batch_request,context= context, suite=suite))\n",
    "            if 'pandas' in data_type: \n",
    "              pandas_batch_request = self._create_batch_request(datasource_name=\"pandas_source\",\n",
    "                                                           data_connector_name=\"PandasDataconnector\", \n",
    "                                                           data_asset_name=f'{rules_source[0][\"source\"]}'+'_pandas',\n",
    "                                                           data_batch=data_df.toPandas(),batch_id=\"pandas_source\", \n",
    "                                                           run_id= run_id) \n",
    "              pandas_suite = context.create_expectation_suite(f'{rules_source[0][\"source\"]}'+'_pandas', overwrite_existing=True)\n",
    "              self._add_expectations_to_suite(data=pandas_data,suite=pandas_suite)\n",
    "              validator.append(self._create_validator(batch_request=pandas_batch_request,context= context, suite=pandas_suite))\n",
    "          except Exception as e:\n",
    "            logger.error(str(e)+\"Exception Occured in the Batch Request Creation or Suite Creation!!!\")\n",
    "          results = context.run_validation_operator(\"action_list_operator\", assets_to_validate=validator,result_format = {'result_format' : \"COMPLETE\",'include_unexpected_rows': True})     \n",
    "          logger.info('results are being Generated...')\n",
    "          logger.info(f\"{results}\")\n",
    "\n",
    "          if results: \n",
    "            eval_rows = data_df.count()\n",
    "            validation_ids = [res for res in results['run_results']]\n",
    "            validations_results_dict = results.to_json_dict()\n",
    "            if validation_ids != []:\n",
    "              bad_rec_count_ret  =self._get_bad_records(validation_ids= validation_ids, validations_results_dict=validations_results_dict, data_df=data_df)\n",
    "              if bad_rec_count_ret== None: bad_rec_count_ret==0\n",
    "              result_format = validations_results_dict['validation_operator_config']['kwargs']['result_format']['result_format']\n",
    "              results_object = []\n",
    "              resultss = validations_results_dict['run_results'][str(validation_ids[0])]['validation_result']\n",
    "              val_time = resultss['meta']['validation_time']\n",
    "              match = re.search(r'\\d{4}\\d{2}\\d{2}', val_time)\n",
    "              val_date = dt.strptime(match.group(), '%Y%m%d').date()\n",
    "              validation_time = val_date.strftime(\"%Y-%m-%d\")\n",
    "              file_name = resultss['results'][0]['expectation_config']['meta']['file_name']\n",
    "              success = resultss['success']\n",
    "              for f in resultss['results']:\n",
    "                if f['expectation_config']['expectation_type'] == \"expect_column_value_to_be_in_normal_range\":\n",
    "                    print(\"inn\")\n",
    "                    f.update({'validation_time' : validation_time})\n",
    "                    results_object.append(f)\n",
    "                else:\n",
    "                    f.update({'result' : {'element_count' : f['result'].get('element_count'),'overall_success':success,'result_format':result_format, 'unexpected_count' : f['result'].get('unexpected_count')},'validation_time' : validation_time})\n",
    "                    results_object.append(f)\n",
    "                    \n",
    "                \n",
    "              resultss['statistics'].update({'success':success, 'validation_time' : validation_time})\n",
    "              resultss['statistics'].update({'unexpected_rows': bad_rec_count_ret,'evaluated_rows':eval_rows,'file_name':file_name})\n",
    "              stat = {'statistics' : resultss['statistics'] }  \n",
    "              \n",
    "              results_object.append(stat)\n",
    "              if len(validation_ids) > 1:\n",
    "                stats = validations_results_dict['run_results'][str(validation_ids[1])]['validation_result']\n",
    "                stats['statistics'].update({'success':stats['success'], 'validation_time' : validation_time,'file_name':file_name})\n",
    "                stat_pandas = {'statistics' : stats['statistics']}  \n",
    "                results_object.append(stat_pandas)\n",
    "              result_path = CONFIG[\"DQ_CHECK_CONFIG\"][\"RESULT_PATH\"]\n",
    "              file = f'{str(result_path)}/result_{str(file_date)}.json'\n",
    "              os.makedirs(os.path.dirname(file), exist_ok=True)\n",
    "              with open(file, 'w') as out:\n",
    "                for objects in results_object:\n",
    "#                    out.write(\"\\n\")\n",
    "                    out.write(json.dumps(objects))\n",
    "                    out.write(\"\\n\")\n",
    "#                     logger.info(json.dumps(objects))\n",
    "#                    print(objects)\n",
    "              logger.info(f\"File {file} Created in the Directory\")\n",
    "          context.build_data_docs() \n",
    "          logger.info(\"DONE!!!\")               \n",
    "      else:\n",
    "          logger.error(\"Rules Repository Don't Have the data for specified file name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8ab5bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "name 'CONFIG' is not definedError Occured in the actual process\n"
     ]
    }
   ],
   "source": [
    "dq_obj = GeDQFrameWork()\n",
    "try:\n",
    "    dq_obj.actual_process(\"movies\")\n",
    "    logger.info(\"Process Completed Successfully!!!\")\n",
    "except Exception as e:\n",
    "    logger.error(str(e)+\"Error Occured in the actual process\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "78e2df4fe7e8030381d9eb3a0cdd9f95f546cd90ef84515903623a27006596e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
