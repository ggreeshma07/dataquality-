{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20388e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\greeshma.ganta\\\\Anaconda3\\\\lib\\\\site-packages\\\\pyspark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import sentry_sdk\n",
    "\n",
    "sentry_sdk.init(\n",
    "    dsn=\"https://examplePublicKey@o0.ingest.sentry.io/0\",\n",
    "\n",
    "    # Set traces_sample_rate to 1.0 to capture 100%\n",
    "    # of transactions for performance monitoring.\n",
    "    # We recommend adjusting this value in production.\n",
    "    traces_sample_rate=1.0\n",
    ")\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b3e18e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"mail_notify\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6bc3250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting great_expectations===0.14.12\n",
      "  Using cached great_expectations-0.14.12-py3-none-any.whl (4.9 MB)\n",
      "Requirement already satisfied: tzlocal>=1.2 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from great_expectations===0.14.12) (4.1)\n",
      "Requirement already satisfied: ipywidgets>=7.5.1 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from great_expectations===0.14.12) (7.6.5)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from great_expectations===0.14.12) (1.1.0)\n",
      "Requirement already satisfied: jinja2<3.1.0,>=2.10 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from great_expectations===0.14.12) (3.0.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from great_expectations===0.14.12) (1.26.7)\n",
      "Requirement already satisfied: tqdm>=4.59.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from great_expectations===0.14.12) (4.62.3)\n",
      "Requirement already satisfied: jsonpatch>=1.22 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from great_expectations===0.14.12) (1.32)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from great_expectations===0.14.12) (2.26.0)\n",
      "Requirement already satisfied: pytz>=2021.3 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from great_expectations===0.14.12) (2021.3)\n",
      "Requirement already satisfied: scipy>=0.19.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from great_expectations===0.14.12) (1.7.1)\n",
      "Requirement already satisfied: Click>=7.1.2 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from great_expectations===0.14.12) (8.0.3)\n",
      "Requirement already satisfied: numpy>=1.14.1 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from great_expectations===0.14.12) (1.20.3)\n",
      "Requirement already satisfied: mistune<2.0.0,>=0.8.4 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from great_expectations===0.14.12) (0.8.4)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from great_expectations===0.14.12) (3.10.0.2)\n",
      "Requirement already satisfied: colorama>=0.4.3 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from great_expectations===0.14.12) (0.4.4)\n",
      "Requirement already satisfied: ruamel.yaml<0.17.18,>=0.16 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from great_expectations===0.14.12) (0.17.17)\n",
      "Requirement already satisfied: importlib-metadata>=1.7.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from great_expectations===0.14.12) (4.8.1)\n",
      "Requirement already satisfied: jsonschema>=2.5.1 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from great_expectations===0.14.12) (3.2.0)\n",
      "Requirement already satisfied: altair<5,>=4.0.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from great_expectations===0.14.12) (4.2.0)\n",
      "Requirement already satisfied: cryptography>=3.2 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from great_expectations===0.14.12) (3.4.8)\n",
      "Requirement already satisfied: nbformat>=5.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from great_expectations===0.14.12) (5.7.0)\n",
      "Requirement already satisfied: pandas>=0.23.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from great_expectations===0.14.12) (1.3.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from great_expectations===0.14.12) (21.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from great_expectations===0.14.12) (2.8.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from great_expectations===0.14.12) (2.4.7)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from altair<5,>=4.0.0->great_expectations===0.14.12) (0.3)\n",
      "Requirement already satisfied: toolz in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from altair<5,>=4.0.0->great_expectations===0.14.12) (0.11.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from cryptography>=3.2->great_expectations===0.14.12) (1.14.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from cffi>=1.12->cryptography>=3.2->great_expectations===0.14.12) (2.20)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from importlib-metadata>=1.7.0->great_expectations===0.14.12) (3.6.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from ipywidgets>=7.5.1->great_expectations===0.14.12) (6.4.1)\n",
      "Requirement already satisfied: ipython>=4.0.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from ipywidgets>=7.5.1->great_expectations===0.14.12) (7.29.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from ipywidgets>=7.5.1->great_expectations===0.14.12) (3.5.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from ipywidgets>=7.5.1->great_expectations===0.14.12) (5.1.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from ipywidgets>=7.5.1->great_expectations===0.14.12) (1.0.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from ipywidgets>=7.5.1->great_expectations===0.14.12) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.1.2)\n",
      "Requirement already satisfied: jupyter-client<8.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->great_expectations===0.14.12) (6.1.12)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->great_expectations===0.14.12) (6.1)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->great_expectations===0.14.12) (1.4.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (3.0.20)\n",
      "Requirement already satisfied: backcall in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.18.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (58.0.4)\n",
      "Requirement already satisfied: pygments in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (2.10.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (5.1.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from jinja2<3.1.0,>=2.10->great_expectations===0.14.12) (2.1.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from jsonpatch>=1.22->great_expectations===0.14.12) (2.2)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from jsonschema>=2.5.1->great_expectations===0.14.12) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from jsonschema>=2.5.1->great_expectations===0.14.12) (21.2.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from jsonschema>=2.5.1->great_expectations===0.14.12) (1.16.0)\n",
      "Requirement already satisfied: pyzmq>=13 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets>=7.5.1->great_expectations===0.14.12) (22.2.1)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets>=7.5.1->great_expectations===0.14.12) (4.8.1)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from jupyter-core>=4.6.0->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets>=7.5.1->great_expectations===0.14.12) (225)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from nbformat>=5.0->great_expectations===0.14.12) (2.16.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.2.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from requests>=2.20->great_expectations===0.14.12) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from requests>=2.20->great_expectations===0.14.12) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from requests>=2.20->great_expectations===0.14.12) (3.2)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.1.2 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from ruamel.yaml<0.17.18,>=0.16->great_expectations===0.14.12) (0.2.6)\n",
      "Requirement already satisfied: pytz-deprecation-shim in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from tzlocal>=1.2->great_expectations===0.14.12) (0.1.0.post0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from tzlocal>=1.2->great_expectations===0.14.12) (2021.5)\n",
      "Requirement already satisfied: notebook>=4.4.1 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (6.5.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.9.4)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.11.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbclassic>=0.4.7 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.4.8)\n",
      "Requirement already satisfied: nbconvert>=5 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (6.5.4)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (20.1.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (1.5.1)\n",
      "Requirement already satisfied: jupyter-server>=1.8 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (1.21.0)\n",
      "Requirement already satisfied: notebook-shim>=0.1.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.2.2)\n",
      "Requirement already satisfied: websocket-client in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (1.4.2)\n",
      "Requirement already satisfied: anyio<4,>=3.1.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (3.6.2)\n",
      "Requirement already satisfied: pywinpty in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.5.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (1.2.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (4.6.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (1.4.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (4.10.0)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (1.2.1)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.1.2)\n",
      "Requirement already satisfied: bleach in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (4.0.0)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.5.3)\n",
      "Requirement already satisfied: async-generator in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from nbclient>=0.5.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (1.10)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (2.2.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\abhinav.anand\\anaconda3\\envs\\pysparkenv\\lib\\site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations===0.14.12) (0.5.1)\n",
      "Installing collected packages: great-expectations\n",
      "  Attempting uninstall: great-expectations\n",
      "    Found existing installation: great-expectations 0.15.31\n",
      "    Uninstalling great-expectations-0.15.31:\n",
      "      Successfully uninstalled great-expectations-0.15.31\n",
      "Successfully installed great-expectations-0.14.12\n"
     ]
    }
   ],
   "source": [
    "!pip install great_expectations===0.14.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac404679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "#sys.path.insert(1,\"C:\\\\Users\\\\abhinav.anand\\\\Desktop\\\\dq_check\")\n",
    "from great_expectations.data_context.types.base import DatasourceConfig\n",
    "from great_expectations.core.batch import RuntimeBatchRequest\n",
    "from great_expectations.data_context.types.base import DataContextConfig, DatasourceConfig, FilesystemStoreBackendDefaults\n",
    "from great_expectations.data_context import BaseDataContext\n",
    "from great_expectations.exceptions import DataContextError\n",
    "from great_expectations.core.expectation_configuration import ExpectationConfiguration\n",
    "from great_expectations.render.renderer import ValidationResultsPageRenderer\n",
    "from ruamel import yaml\n",
    "from multiprocessing.pool import Pool, ThreadPool\n",
    "import multiprocessing\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import concat_ws\n",
    "from pyspark.sql.window import Window\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import datetime\n",
    "import configparser\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abb4458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run \"C:/Users/abhinav.anand/Desktop/dq_check/custom_expectations/expect_column_values_to_contain_valid_email.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31ccaca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run \"C:/Users/abhinav.anand/Desktop/dq_check/custom_expectations/expect_column_values_to_not_contain_special_characters.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25395d45",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (Temp/ipykernel_12448/400148552.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\GREESH~1.GAN\\AppData\\Local\\Temp/ipykernel_12448/400148552.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    CONFIG.read(\"C:\\Users\\greeshma.ganta\\Desktop\\dqc/config/config.ini\")\u001b[0m\n\u001b[1;37m                                                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "CONFIG= configparser.ConfigParser()\n",
    "CONFIG.read(\"C:\\Users\\greeshma.ganta\\Desktop\\dqc/config/config.ini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45b43aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/abhinav.anand/Desktop/rules.json\n"
     ]
    }
   ],
   "source": [
    "print(CONFIG[\"DQ_CHECK_CONFIG\"][\"RULES_REPO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcf6e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_date = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "log_file='C:/Users/abhinav.anand/Desktop/dq_check/logs/'+'dq_ge_'+str(file_date)+'.log'\n",
    "format_log = logging.Formatter('%(asctime)s  %(levelname)-8s [%(name)s:%(lineno)d]  %(message)s')\n",
    "\n",
    "logger = logging.getLogger('dq_ge')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "fh = logging.FileHandler(log_file, mode='a')\n",
    "\n",
    "sh = logging.StreamHandler()\n",
    "sh.setLevel(logging.DEBUG)\n",
    "\n",
    "fh.setFormatter(format_log)\n",
    "sh.setFormatter(format_log)\n",
    "if (logger.hasHandlers()):\n",
    "  logger.handlers.clear()\n",
    "logger.addHandler(sh)\n",
    "logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d97eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeDQFrameWork:\n",
    "  \n",
    "    #Create_data_source_config\n",
    "    def _data_source_config(self,connector, batch_id, engine):\n",
    "      data_source_config = DatasourceConfig(\n",
    "          class_name=\"Datasource\",\n",
    "          execution_engine={\"class_name\": engine },\n",
    "          data_connectors={\n",
    "            connector : {\n",
    "              \"module_name\": \"great_expectations.datasource.data_connector\",\n",
    "              \"class_name\": \"RuntimeDataConnector\",\n",
    "              \"batch_identifiers\": [str(batch_id)] }\n",
    "          }\n",
    "        )\n",
    "      logger.info(f'{connector} - Datasource Genereted')\n",
    "      return data_source_config\n",
    "\n",
    "    \n",
    "    #Create_project_config\n",
    "    def _create_project_config(self, spark_datasource_name, spark_data_source_config,pandas_datasource_name, pandas_data_source_config):\n",
    "        project_config = DataContextConfig( datasources={\n",
    "            spark_datasource_name : spark_data_source_config, pandas_datasource_name:pandas_data_source_config}, \n",
    "                                           stores={ \"expectations_store\": {\n",
    "                  \"class_name\": \"ExpectationsStore\",\n",
    "                  \"store_backend\": { \"class_name\": \"TupleFilesystemStoreBackend\",\n",
    "                  \"base_directory\": CONFIG[\"DQ_CHECK_CONFIG\"][\"EXPECTATION_PATH\"]}, },\n",
    "                  \"validations_store\": {\n",
    "                      \"class_name\": \"ValidationsStore\",\n",
    "                      \"store_backend\": { \"class_name\": \"TupleFilesystemStoreBackend\",\n",
    "                      \"base_directory\": CONFIG[\"DQ_CHECK_CONFIG\"][\"VALIDATION_PATH\"],\n",
    "                        },},\n",
    "                   \"evaluation_parameter_store\": { \"class_name\": \"EvaluationParameterStore\"},\n",
    "                                                    \n",
    "              }, \n",
    "        validation_operators={ \"action_list_operator\": {\n",
    "                \"class_name\": \"ActionListValidationOperator\",\n",
    "                \"action_list\": [{\n",
    "                        \"name\": \"store_validation_result\",\n",
    "                        \"action\": {\"class_name\": \"StoreValidationResultAction\"},\n",
    "                    },\n",
    "                   {\n",
    "                         \"name\": \"store_evaluation_params\",\n",
    "                         \"action\": {\"class_name\": \"StoreEvaluationParametersAction\"},\n",
    "                     },\n",
    "                     {\n",
    "                        \"name\": \"update_data_docs\",\n",
    "                        \"action\": {\"class_name\": \"UpdateDataDocsAction\"},\n",
    "                    }\n",
    "#                     {\n",
    "#                       \"name\": \"send_email_on_validation_result\",\n",
    "#                       \"action\": {\n",
    "#                         \"class_name\": \"EmailAction\",\n",
    "#                         \"notify_on\": \"all\",\n",
    "#                         \"use_tls\": True,\n",
    "#                         \"use_ssl\": False,\n",
    "#                         \"renderer\": {\n",
    "#                           \"module_name\": \"great_expectations.render.renderer.email_renderer\",\n",
    "#                           \"class_name\": \"EmailRenderer\"\n",
    "#                         },\n",
    "#                         \"smtp_address\": \"smtp-mail.outlook.com\",\n",
    "#                         \"smtp_port\": 587,\n",
    "#                         \"sender_login\": \"abhinavv.anand@outlook.com\",\n",
    "#                         \"sender_password\": \"****\",\n",
    "#                         \"sender_alias\": \"abhinavv.anand@outlook.com\",\n",
    "#                         \"receiver_emails\": \"abhinavanandpnbe@gmail.com\",\n",
    "#                       }\n",
    "#                     }\n",
    "                     \n",
    "                ]} },\n",
    "           data_docs_sites = {\n",
    "                    \"local_site\": {\n",
    "                    \"class_name\": \"SiteBuilder\",\n",
    "                    \"store_backend\": { \"class_name\": \"TupleFilesystemStoreBackend\",\n",
    "                                     \"base_directory\": CONFIG[\"DQ_CHECK_CONFIG\"][\"DATA_DOCS_PATH\"]},\n",
    "                    \"site_index_builder\": {\"class_name\": \"DefaultSiteIndexBuilder\",\n",
    "                                     \"show_cta_footer\": True, },} },\n",
    "                                           \n",
    "             store_backend_defaults=FilesystemStoreBackendDefaults(root_directory=CONFIG[\"DQ_CHECK_CONFIG\"][\"VALIDATION_PATH\"])\n",
    "            )\n",
    "        logger.info('Project Configuration Created')\n",
    "        return project_config\n",
    "\n",
    "    #create_batch_request\n",
    "    def _create_batch_request(self, datasource_name, data_connector_name, data_asset_name,data_batch,batch_id, run_id):\n",
    "      batch_request = RuntimeBatchRequest(\n",
    "        datasource_name= datasource_name,\n",
    "        data_connector_name= data_connector_name,\n",
    "        data_asset_name= data_asset_name,\n",
    "        runtime_parameters= { \"batch_data\": data_batch },\n",
    "        batch_identifiers= {batch_id: run_id})\n",
    "      logger.info(f'{datasource_name} - Batchrequest Created !!!')\n",
    "      return batch_request\n",
    "\n",
    "    #add_expectations_to_the_suite\n",
    "    def _add_expectations_to_suite(self,data,suite):\n",
    "      data_meta = None\n",
    "      for row in data.collect():\n",
    "        if row['source_type'] == 'csv': \n",
    "          data_meta = {\"file_name\": row[\"source\"]}\n",
    "        kwargs = json.loads('{'+str(row[\"parameter\"])+'}')\n",
    "        expectation_configuration = ExpectationConfiguration(expectation_type = row[\"validation_name\"], kwargs =kwargs, meta = data_meta)\n",
    "        suite.add_expectation(expectation_configuration=expectation_configuration,match_type=\"domain\")\n",
    "        logger.info(f'Expectation - {row[\"validation_name\"]} - Created for:  {row[\"source\"]}')\n",
    "      return \n",
    "    \n",
    "    #create_validadtor\n",
    "    def _create_validator(self, batch_request,context, suite):\n",
    "      data_validator = context.get_validator(batch_request=batch_request,expectation_suite=suite)\n",
    "      logger.info(\"Validator Created\")\n",
    "      data_validator.save_expectation_suite(discard_failed_expectations=False)\n",
    "      return data_validator\n",
    "\n",
    "    #read_dataframe\n",
    "    def _read_data_frame(self, path):\n",
    "      data_frame = spark.read.format(\"csv\").options(inferSchema = \"true\" ,multiline = \"true\",header = \"true\", sep= \",\",quote=\"\\\"\",escape=\"\\\"\").load(path)\n",
    "      return data_frame\n",
    "#     def _read_data_json(self, path):\n",
    "#     # Opening JSON file\n",
    "#       f = open(path)\n",
    "#       dataf = json.load(f)\n",
    "#       return dataf\n",
    "    \n",
    "    \n",
    "    def _get_failed_expectations(self,resultss,df):\n",
    "      failed_expectations = list()\n",
    "      if isinstance(resultss, list):\n",
    "          failed_expectations = list(__builtins__.filter(lambda x: x['success']== False, resultss))\n",
    "      return self._bad_records_count(failed_expectations,df)\n",
    "        \n",
    "\n",
    "    def _bad_records_count(self,failed_expectations,df):\n",
    "        bad_records_df = df.filter(\"1 == 2\")##check\n",
    "        good_records_df_count = None\n",
    "        for f in failed_expectations:\n",
    "            column_name = f['expectation_config']['kwargs'].get('column')\n",
    "            unexpected_list = f['result'].get('unexpected_list')\n",
    "\n",
    "            if f['expectation_config'].get('expectation_type') == 'expect_column_values_to_not_be_null':\n",
    "                data_df = df.filter(col(column_name).isNull())\n",
    "\n",
    "\n",
    "            if column_name is not None:\n",
    "                if unexpected_list is not None and len(unexpected_list) > 0:   \n",
    "                    temp_df = df.filter(col(column_name).isin(set(unexpected_list)))\n",
    "                    if f['expectation_config'].get('expectation_type') == 'expect_column_values_to_not_be_null':\n",
    "                        temp_df = data_df\n",
    "                        \n",
    "                    #data_df.show() \n",
    "                    bad_records_df = bad_records_df.union(temp_df)\n",
    "                    good_records_df = df.subtract(bad_records_df)\n",
    "                    try:\n",
    "                        if good_records_df.count() > 0:\n",
    "#                            good_records_df_count = good_records_df.dropDuplicates().count()\n",
    "                            good_records_df_count = good_records_df.dropDuplicates().count()\n",
    "                            #logger.info(f'Good Records Count: {good_records_df_count}')\n",
    "                    except Exception as e:\n",
    "                        logger.error(f'{e} Exception occured in Getting good records')\n",
    "#         bad_records = bad_records_df.coalesce(1).write.option(\"header\", \"true\").mode(\"append\").csv(\"C:/Users/abhinav.anand/Desktop/dq_check/\"+\"bad_rec\"+'_'+str(file_date)+\".csv\")\n",
    "#         good_records = good_records_df.coalesce(1).write.option(\"header\", \"true\").mode(\"append\").csv(\"C:/Users/abhinav.anand/Desktop/dq_check/\"+\"good_rec\"+'_'+str(file_date)+\".csv\")            \n",
    "        logger.info(f'Good Records Count: {good_records_df_count}')         \n",
    "        if bad_records_df.count()   > 0:\n",
    "            bad_records_df_count = bad_records_df.count()\n",
    "#            bad_records_df_count = bad_records_df.dropDuplicates().count()\n",
    "            \n",
    "            return bad_records_df_count\n",
    "    \n",
    "          \n",
    "    def _get_bad_records(self,validation_ids, validations_results_dict, data_df):\n",
    "              results_dict = None\n",
    "              bad_rec_count = None\n",
    "              for ind,val in enumerate(validation_ids): \n",
    "                logger.info(f'Validation Id:{val}')\n",
    "                if ind == 0:\n",
    "                    results_dict = validations_results_dict['run_results'][str(val)]['validation_result']['results']\n",
    "                if ind >= 1:\n",
    "                    for i in validations_results_dict['run_results'][str(val)]['validation_result']['results']:\n",
    "                        results_dict.append(i)\n",
    "              try:\n",
    "                if results_dict != None:\n",
    "                  bad_rec_count = self._get_failed_expectations(resultss=results_dict, df=data_df)\n",
    "                  logger.info(f'Bad Records Count: {bad_rec_count}')\n",
    "              except Exception as e:\n",
    "                logger.error(f'{e} Exception occured in Getting bad records')\n",
    "              return bad_rec_count\n",
    "\n",
    "    \n",
    "    #Mandatory_optional_field_checking\n",
    "    #Mandatory_optional_field_checking\n",
    "    def _mandatory_option_check(self, joined_table):\n",
    "        df_list=[]\n",
    "        combinedDF = joined_table.select( col(\"id\"),col(\"parameter\"), col(\"parameter_name\"),col(\"parameter_required\"))\n",
    "        df1 = combinedDF.withColumn('new', concat_ws(',',split(regexp_replace(regexp_replace(col('parameter'),' ',''),'\"',\"'\"), ':')))\n",
    "        quoted_page_name = concat(lit(\"'\"), col(\"parameter_name\"), lit(\"'\"))\n",
    "        res = df1.withColumn(\"IDmatch\", when(col(\"new\").contains(quoted_page_name), True).otherwise(False))\n",
    "        res.withColumn(\"size\",when(size(split(regexp_replace(col(\"parameter\"),':([^,]*)', \"~#~$1\"),'~#~'))==1,1).otherwise(size(split(regexp_replace(col(\"parameter\"),':([^,]*)', \"~#~$1\"),'~#~'))-1)).registerTempTable('res')\n",
    "        if (res.filter((res.IDmatch == \"False\") & (res.parameter_required == 'MANDATORY')).count() >0 ):\n",
    "            df_list.append(res.select(col(\"id\"),lit('MANDATORY').alias('error')).filter((res.IDmatch == \"False\") & (res.parameter_required == 'MANDATORY') ))\n",
    "        if spark.sql(\"select id,IDmatch,size from res group by id,IDmatch,size having count(*) != size and IDmatch == 'true'\").count() > 0:\n",
    "            df_list.append(spark.sql(\"select id ,'OPTIONAL' from res group by id,IDmatch,size having count(*) != size and IDmatch == 'true'\"))\n",
    "        return df_list\n",
    "    \n",
    "    def _check_column(self,source_table,combined_table):\n",
    "      list_1 = []\n",
    "      combined_table = combined_table.withColumn(\"table_columns\",lit(','.join(source_table.columns)))\n",
    "      if combined_table.filter(col(\"parameter\").contains('\"column\"')).count() >0 :\n",
    "          combined_table_filterd_column = combined_table.filter(col(\"parameter\").contains('\"column\"'))\n",
    "          combined_table_filterd_column = combined_table_filterd_column.withColumn('new', regexp_replace(regexp_replace(split(split('parameter',',')[0],':')[1],'\"',''),' ',''))\n",
    "          res = combined_table_filterd_column.withColumn(\"column_match\", when(col(\"table_columns\").contains(col(\"new\")), True).otherwise(False))      \n",
    "          if res.filter(col(\"column_match\") == False).count() >0: list_1.append(res.filter(col(\"column_match\") == False))\n",
    "      if combined_table.filter(col(\"parameter\").contains('\"column_list\"')).count() >0 :\n",
    "          combined_table_filterd_column_list = combined_table.filter(col(\"parameter\").contains('\"column_list\"'))\n",
    "          combined_table_filterd_column_list = combined_table_filterd_column_list.withColumn('new',explode(split(regexp_replace(regexp_replace(regexp_replace(regexp_replace(split(col('parameter'),':')[1],'\"',\"\"),'[',''),']',''),' ',''),',')))\n",
    "          res_column_list = combined_table_filterd_column_list.withColumn(\"column_match\", when(col(\"table_columns\").contains(col(\"new\")), True).otherwise(False))\n",
    "          if res_column_list.filter(col(\"column_match\") == False).count() >0: list_1.append(res_column_list.filter(col(\"column_match\") == False))\n",
    "      if combined_table.filter(col(\"parameter\").contains('\"column_A\"')).count() >0 :\n",
    "          combined_table_filterd_column_mul = combined_table.filter(col(\"parameter\").contains('\"column_A\"') | col(\"parameter\").contains('\"column_B\"'))\n",
    "          combined_table_filterd_column_mul = combined_table_filterd_column_mul.withColumn('new',explode(split(regexp_replace(regexp_replace(concat(split(split('parameter',',')[0],':')[1],lit(\"~#~\"),split(split('parameter',',')[1],\":\")[1]),'\"',''),' ',''),'~#~')))\n",
    "          res_column_mul = combined_table_filterd_column_mul.withColumn(\"column_match\", when(col(\"table_columns\").contains(col(\"new\")), True).otherwise(False))\n",
    "          if res_column_mul.filter(col(\"column_match\") == False).count() >0: list_1.append(res_column_mul.filter(col(\"column_match\") == False))\n",
    "      return list_1\n",
    "    \n",
    "    \n",
    "    \n",
    "    #read_rules_repository\n",
    "    def _get_rules_repo(self, file_name):\n",
    "      logger.info(\"Read Rules Repository Files - Started\")\n",
    "      dq_validation = self._read_data_frame(path=CONFIG[\"DQ_CHECK_CONFIG\"][\"DQ_VALIDATION\"])\n",
    "      source_validation = self._read_data_frame(path=CONFIG[\"DQ_CHECK_CONFIG\"][\"SOURCE_VALIDATION\"])\n",
    "      source_meta = self._read_data_frame(path=CONFIG[\"DQ_CHECK_CONFIG\"][\"SOURCE_META\"])\n",
    "      joined_table = self._read_data_frame(path=CONFIG[\"DQ_CHECK_CONFIG\"][\"RULES_REPO\"])\n",
    "      validation_parameter = self._read_data_frame(path=CONFIG[\"DQ_CHECK_CONFIG\"][\"VALIDATION_LIBRARY_PARAMETER\"])\n",
    "      joined_table = source_validation.alias(\"sv\").join(dq_validation.alias('dv'), source_validation.dqvalidation_id == dq_validation.id,\"inner\").join(\n",
    "          source_meta.alias('sm'),source_validation.source_meta_id == source_meta.id,\"left\")\n",
    "      mandatory_optional_check = source_validation.alias(\"sv\").join(\n",
    "         validation_parameter.alias('vp'),validation_parameter.dqvalidation_id == source_validation.dqvalidation_id,\"left\")\n",
    "      man_df = self._mandatory_option_check(mandatory_optional_check)\n",
    "      if len(man_df) == 0: \n",
    "          logger.info('Mandatory Optional Check Completed')\n",
    "      else: \n",
    "        logger.info('Issue in Mandatory Optional Check...')\n",
    "        raise Exception(\"Parameter missing or wrong entry in the rule repo\")\n",
    "      add_db_table = joined_table.withColumn(\"source\", concat_ws(\"\",col('file_name')))\n",
    "      db_tables = add_db_table.filter(add_db_table.source == str(f'{file_name}'))\n",
    "\n",
    "      return db_tables\n",
    "    \n",
    "\n",
    "    def _pre_process(self,df,validation_data):\n",
    "      config_data = {}\n",
    "      list_return=[]\n",
    "      for (each_key, each_val) in CONFIG.items(\"PRE_PROCESS_CONFIG\"):\n",
    "        config_data[each_key] = each_val\n",
    "        pre_process_list = validation_data.select('parameter','validation_name').collect()\n",
    "        for pre_process_item in pre_process_list:\n",
    "          if pre_process_item['validation_name'] in config_data.keys() :\n",
    "            column_name = str(pre_process_item['parameter'].split(',')[0].split(':')[1]).replace('\"','').replace(' ','')\n",
    "            if df.schema[column_name].dataType != 'DoubleType' and config_data[pre_process_item['validation_name']]=='DoubleType':\n",
    "              df=df.withColumn(column_name, df[column_name].cast(DoubleType()))\n",
    "            if df.schema[column_name].dataType != 'IntegerType' and config_data[pre_process_item['validation_name']]=='IntegerType':\n",
    "              df=df.withColumn(column_name, df[column_name].cast(IntegerType()))\n",
    "      list_return.append(df)\n",
    "      if validation_data.filter(col(\"validation_name\")=='expect_column_values_to_change_between').count()>0:\n",
    "        data_list=validation_data.filter(col(\"validation_name\")=='expect_column_values_to_change_between').collect()\n",
    "        expectaion=data_list[0]\n",
    "        column_name = (((expectaion['parameter'].split(',')[0]).split(':')[1]).replace('\"','')).replace(' ','')\n",
    "        from_val = (((expectaion['parameter'].split(',')[1]).split(':')[1]).replace('\"','')).replace(' ','')\n",
    "        to_val = (((expectaion['parameter'].split(',')[2]).split(':')[1]).replace('\"','')).replace(' ','')\n",
    "        my_window = Window.partitionBy().orderBy(column_name)\n",
    "        df = df.withColumn(column_name+\"_diff\", lead(col(column_name)).over(my_window))\n",
    "        df=df.withColumn(column_name+\"_diff\",col(column_name+\"_diff\")-col(column_name))\n",
    "        param = '\"column\" : \"'+column_name+'_diff\" , \"min_value\" : '+str(from_val)+' , \"max_value\" : '+str(to_val)\n",
    "        validation_data=validation_data.withColumn('parameter', regexp_replace(col('parameter'),str(expectaion['parameter']), param))\n",
    "        validation_data=validation_data.withColumn('validation_name', regexp_replace(col('validation_name'),str(expectaion['validation_name']), 'expect_column_values_to_be_between'))\n",
    "        validation_data=validation_data.withColumn('type',when(col(\"validation_name\") == lit('expect_column_values_to_be_between'),regexp_replace(col(\"type\"),'pandas','spark')).otherwise(col(\"type\")))\n",
    "        list_return[0]=df\n",
    "        list_return.append(validation_data)\n",
    "      return list_return\n",
    "    \n",
    "    def actual_process(self,file_name):\n",
    "      logger.info(\"Framework has been Started\")\n",
    "      logger.info(\"actual process has been triggered\")\n",
    "        \n",
    "      \"\"\"\n",
    "      step 1: Read rules repository\n",
    "      step 2: initialize dataContext with Project configuration\n",
    "      step 3: Batch creation\n",
    "      step 4: Create expectation Suite\n",
    "      step 5: Add expectations to the Expectation suite\n",
    "      step 6: Creation of Validator\n",
    "      step 7: Validate data\n",
    "      \"\"\"\n",
    "      \n",
    "      rules_repo = self._get_rules_repo(file_name)\n",
    "      logger.info(f\"Rules Repository Dataframe Generated\")\n",
    "      if rules_repo.rdd.isEmpty() == False:\n",
    "          logger.info(\"Rules Repository Dataframe Have the data\")\n",
    "          spark_datasource_config = self._data_source_config(connector= \"SparkDataconnector\",\n",
    "                                                             batch_id = \"spark_source\",engine = \"SparkDFExecutionEngine\")\n",
    "          pandas_datasource_config = self._data_source_config(connector= \"PandasDataconnector\",\n",
    "                                                              batch_id = \"pandas_source\",engine = \"PandasExecutionEngine\") \n",
    "\n",
    "          project_config=self._create_project_config(spark_datasource_name=\"spark_source\", \n",
    "                                                     spark_data_source_config=spark_datasource_config,\n",
    "                                                     pandas_datasource_name= \"pandas_source\", pandas_data_source_config = pandas_datasource_config)\n",
    "          context = BaseDataContext(project_config=project_config)\n",
    "          logger.info(\"Context Created with Project Configuration\")\n",
    "          rules_source = rules_repo.dropDuplicates(['source']).collect()\n",
    "          run_id = f'{datetime.datetime.now()}'\n",
    "          try:\n",
    "            if rules_source[0]['source_type'] == 'table':\n",
    "              logger.info(\"Source type - Table\")\n",
    "              data_df = spark.table(rules_source[0]['source'])\n",
    "              check_column = self._check_column(data_df,rules_repo)\n",
    "              if len(check_column) ==0: logger.info(\"Column Check Completed\")                         \n",
    "              else: raise Exception(\"Column not present in the Source Table\")\n",
    "              logger.info(\"PreProcessing Started\")\n",
    "              data_list = self._pre_process(data_df,rules_repo)\n",
    "              if len(data_list)==2 : \n",
    "                data_df = data_list[0]\n",
    "                rules_repo = data_list[1]\n",
    "              elif len(data_list)==1 :\n",
    "                data_df = data_list[0]\n",
    "              logger.info(\"PreProcessing Completed\")\n",
    "            elif rules_source[0]['source_type'] == 'csv':\n",
    "              data_df = spark.read.format(rules_source[0]['source_type']).options(inferSchema = \"true\",header = \"true\", sep= \",\",quote=\"\\\"\",\n",
    "                                                                                  escape=\"\\\"\").load(rules_source[0]['source_path'])\n",
    "          except Exception as e:\n",
    "            logger.error(f\"{e} - Exception Occured in Reading the Source data\")\n",
    "            raise Exception(\"Exit\")\n",
    "          data_type = [i['type'] for i in rules_repo.select(col('type')).collect()]\n",
    "          spark_data = rules_repo.filter(col('type') == 'spark')\n",
    "          pandas_data = rules_repo.filter(col('type') == 'pandas')\n",
    "          validator = []\n",
    "          try:\n",
    "            if 'spark' in data_type: \n",
    "              batch_request = self._create_batch_request(datasource_name=\"spark_source\",\n",
    "                                                         data_connector_name=\"SparkDataconnector\", \n",
    "                                                         data_asset_name=rules_source[0]['source'],\n",
    "                                                         data_batch=data_df,batch_id=\"spark_source\", \n",
    "                                                         run_id= run_id)\n",
    "              suite = context.create_expectation_suite(rules_source[0]['source'], overwrite_existing=True)\n",
    "              logger.info(f'{rules_source[0][\"source\"]} - Spark Expectation Suite Created !!!')\n",
    "              self._add_expectations_to_suite(data=spark_data,suite=suite)\n",
    "              validator.append(self._create_validator(batch_request=batch_request,context= context, suite=suite))\n",
    "            if 'pandas' in data_type: \n",
    "              pandas_batch_request = self._create_batch_request(datasource_name=\"pandas_source\",\n",
    "                                                           data_connector_name=\"PandasDataconnector\", \n",
    "                                                           data_asset_name=f'{rules_source[0][\"source\"]}'+'_pandas',\n",
    "                                                           data_batch=data_df.toPandas(),batch_id=\"pandas_source\", \n",
    "                                                           run_id= run_id) \n",
    "              pandas_suite = context.create_expectation_suite(f'{rules_source[0][\"source\"]}'+'_pandas', overwrite_existing=True)\n",
    "              self._add_expectations_to_suite(data=pandas_data,suite=pandas_suite)\n",
    "              validator.append(self._create_validator(batch_request=pandas_batch_request,context= context, suite=pandas_suite))\n",
    "          except Exception as e:\n",
    "            logger.error(str(e)+\"Exception Occured in the Batch Request Creation or Suite Creation!!!\")\n",
    "          results = context.run_validation_operator(\"action_list_operator\", assets_to_validate=validator,result_format = {'result_format' : \"COMPLETE\",'include_unexpected_rows': True})     \n",
    "          logger.info('results are being Generated...')\n",
    "          logger.info(f\"{results}\")\n",
    "\n",
    "          if results: \n",
    "            eval_rows = data_df.count()\n",
    "            validation_ids = [res for res in results['run_results']]\n",
    "            validations_results_dict = results.to_json_dict()\n",
    "            if validation_ids != []:\n",
    "              bad_rec_count_ret  =self._get_bad_records(validation_ids= validation_ids, validations_results_dict=validations_results_dict, data_df=data_df)\n",
    "              if bad_rec_count_ret== None: bad_rec_count_ret==0\n",
    "              result_format = validations_results_dict['validation_operator_config']['kwargs']['result_format']['result_format']\n",
    "              results_object = []\n",
    "              resultss = validations_results_dict['run_results'][str(validation_ids[0])]['validation_result']\n",
    "              val_time = resultss['meta']['validation_time']\n",
    "              match = re.search(r'\\d{4}\\d{2}\\d{2}', val_time)\n",
    "              val_date = dt.strptime(match.group(), '%Y%m%d').date()\n",
    "              validation_time = val_date.strftime(\"%Y-%m-%d\")\n",
    "              file_name = resultss['results'][0]['expectation_config']['meta']['file_name']\n",
    "              success = resultss['success']\n",
    "              for f in resultss['results']:\n",
    "                if f['expectation_config']['expectation_type'] == \"expect_column_value_to_be_in_normal_range\":\n",
    "                    print(\"inn\")\n",
    "                    f.update({'validation_time' : validation_time})\n",
    "                    results_object.append(f)\n",
    "                else:\n",
    "                    f.update({'result' : {'element_count' : f['result'].get('element_count'),'overall_success':success,'result_format':result_format, 'unexpected_count' : f['result'].get('unexpected_count')},'validation_time' : validation_time})\n",
    "                    results_object.append(f)\n",
    "                    \n",
    "                \n",
    "              resultss['statistics'].update({'success':success, 'validation_time' : validation_time})\n",
    "              resultss['statistics'].update({'unexpected_rows': bad_rec_count_ret,'evaluated_rows':eval_rows,'file_name':file_name})\n",
    "              stat = {'statistics' : resultss['statistics'] }  \n",
    "              \n",
    "              results_object.append(stat)\n",
    "              if len(validation_ids) > 1:\n",
    "                stats = validations_results_dict['run_results'][str(validation_ids[1])]['validation_result']\n",
    "                stats['statistics'].update({'success':stats['success'], 'validation_time' : validation_time,'file_name':file_name})\n",
    "                stat_pandas = {'statistics' : stats['statistics']}  \n",
    "                results_object.append(stat_pandas)\n",
    "              result_path = CONFIG[\"DQ_CHECK_CONFIG\"][\"RESULT_PATH\"]\n",
    "              file = f'{str(result_path)}/result_{str(file_date)}.json'\n",
    "              os.makedirs(os.path.dirname(file), exist_ok=True)\n",
    "              with open(file, 'w') as out:\n",
    "                for objects in results_object:\n",
    "#                    out.write(\"\\n\")\n",
    "                    out.write(json.dumps(objects))\n",
    "                    out.write(\"\\n\")\n",
    "#                     logger.info(json.dumps(objects))\n",
    "#                    print(objects)\n",
    "              logger.info(f\"File {file} Created in the Directory\")\n",
    "          context.build_data_docs() \n",
    "          logger.info(\"DONE!!!\")               \n",
    "      else:\n",
    "          logger.error(\"Rules Repository Don't Have the data for specified file name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8ab5bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 11:17:37,781  INFO     [dq_ge:280]  Framework has been Started\n",
      "2022-11-22 11:17:37,783  INFO     [dq_ge:281]  actual process has been triggered\n",
      "2022-11-22 11:17:37,786  INFO     [dq_ge:226]  Read Rules Repository Files - Started\n",
      "2022-11-22 11:17:40,820  INFO     [dq_ge:238]  Mandatory Optional Check Completed\n",
      "2022-11-22 11:17:40,858  INFO     [dq_ge:294]  Rules Repository Dataframe Generated\n",
      "2022-11-22 11:17:41,779  INFO     [dq_ge:296]  Rules Repository Dataframe Have the data\n",
      "2022-11-22 11:17:41,779  INFO     [dq_ge:15]  SparkDataconnector - Datasource Genereted\n",
      "2022-11-22 11:17:41,782  INFO     [dq_ge:15]  PandasDataconnector - Datasource Genereted\n",
      "2022-11-22 11:17:41,782  INFO     [dq_ge:80]  Project Configuration Created\n",
      "2022-11-22 11:17:41,981  INFO     [dq_ge:306]  Context Created with Project Configuration\n",
      "2022-11-22 11:17:43,525  INFO     [dq_ge:91]  spark_source - Batchrequest Created !!!\n",
      "2022-11-22 11:17:43,543  INFO     [dq_ge:342]  movies - Spark Expectation Suite Created !!!\n",
      "2022-11-22 11:17:43,819  INFO     [dq_ge:103]  Expectation - expect_column_unique_value_count_to_be_between - Created for:  movies\n",
      "2022-11-22 11:17:43,888  INFO     [dq_ge:109]  Validator Created\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30faac61f4f2483d805c3bc6fe37629d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 11:17:50,442  INFO     [dq_ge:357]  results are being Generated...\n",
      "2022-11-22 11:17:50,444  INFO     [dq_ge:358]  {\n",
      "  \"validation_operator_config\": {\n",
      "    \"class_name\": \"ActionListValidationOperator\",\n",
      "    \"module_name\": \"great_expectations.validation_operators\",\n",
      "    \"name\": \"action_list_operator\",\n",
      "    \"kwargs\": {\n",
      "      \"action_list\": [\n",
      "        {\n",
      "          \"name\": \"store_validation_result\",\n",
      "          \"action\": {\n",
      "            \"class_name\": \"StoreValidationResultAction\"\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"store_evaluation_params\",\n",
      "          \"action\": {\n",
      "            \"class_name\": \"StoreEvaluationParametersAction\"\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"update_data_docs\",\n",
      "          \"action\": {\n",
      "            \"class_name\": \"UpdateDataDocsAction\"\n",
      "          }\n",
      "        }\n",
      "      ],\n",
      "      \"result_format\": {\n",
      "        \"result_format\": \"SUMMARY\",\n",
      "        \"partial_unexpected_count\": 20\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"evaluation_parameters\": null,\n",
      "  \"run_id\": {\n",
      "    \"run_time\": \"2022-11-22T05:47:43.979136+00:00\",\n",
      "    \"run_name\": \"20221122T054743.979136Z\"\n",
      "  },\n",
      "  \"success\": true,\n",
      "  \"run_results\": {\n",
      "    \"ValidationResultIdentifier::movies/20221122T054743.979136Z/20221122T054743.979136Z/2c7d67632e907942c3fcebd7c48acdac\": {\n",
      "      \"validation_result\": {\n",
      "        \"statistics\": {\n",
      "          \"evaluated_expectations\": 1,\n",
      "          \"successful_expectations\": 1,\n",
      "          \"unsuccessful_expectations\": 0,\n",
      "          \"success_percent\": 100.0\n",
      "        },\n",
      "        \"evaluation_parameters\": {},\n",
      "        \"meta\": {\n",
      "          \"great_expectations_version\": \"0.14.12\",\n",
      "          \"expectation_suite_name\": \"movies\",\n",
      "          \"run_id\": {\n",
      "            \"run_time\": \"2022-11-22T05:47:43.979136+00:00\",\n",
      "            \"run_name\": \"20221122T054743.979136Z\"\n",
      "          },\n",
      "          \"batch_spec\": {\n",
      "            \"data_asset_name\": \"movies\",\n",
      "            \"batch_data\": \"SparkDataFrame\"\n",
      "          },\n",
      "          \"batch_markers\": {\n",
      "            \"ge_load_time\": \"20221122T054743.826348Z\"\n",
      "          },\n",
      "          \"active_batch_definition\": {\n",
      "            \"datasource_name\": \"spark_source\",\n",
      "            \"data_connector_name\": \"SparkDataconnector\",\n",
      "            \"data_asset_name\": \"movies\",\n",
      "            \"batch_identifiers\": {\n",
      "              \"spark_source\": \"2022-11-22 11:17:42.869063\"\n",
      "            }\n",
      "          },\n",
      "          \"validation_time\": \"20221122T054743.982237Z\"\n",
      "        },\n",
      "        \"success\": true,\n",
      "        \"results\": [\n",
      "          {\n",
      "            \"expectation_config\": {\n",
      "              \"kwargs\": {\n",
      "                \"column\": \"Year\",\n",
      "                \"min_value\": 0,\n",
      "                \"max_value\": 5,\n",
      "                \"batch_id\": \"2c7d67632e907942c3fcebd7c48acdac\"\n",
      "              },\n",
      "              \"meta\": {\n",
      "                \"file_name\": \"movies\"\n",
      "              },\n",
      "              \"expectation_type\": \"expect_column_unique_value_count_to_be_between\"\n",
      "            },\n",
      "            \"result\": {\n",
      "              \"observed_value\": 5\n",
      "            },\n",
      "            \"exception_info\": {\n",
      "              \"raised_exception\": false,\n",
      "              \"exception_traceback\": null,\n",
      "              \"exception_message\": null\n",
      "            },\n",
      "            \"meta\": {},\n",
      "            \"success\": true\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"actions_results\": {\n",
      "        \"store_validation_result\": {\n",
      "          \"class\": \"StoreValidationResultAction\"\n",
      "        },\n",
      "        \"store_evaluation_params\": {\n",
      "          \"class\": \"StoreEvaluationParametersAction\"\n",
      "        },\n",
      "        \"update_data_docs\": {\n",
      "          \"local_site\": \"file://C:/Users/abhinav.anand/Desktop/dq_check/Data_docs/validations\\\\movies\\\\20221122T054743.979136Z\\\\20221122T054743.979136Z\\\\2c7d67632e907942c3fcebd7c48acdac.html\",\n",
      "          \"class\": \"UpdateDataDocsAction\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "2022-11-22 11:17:50,526  INFO     [dq_ge:172]  Validation Id:ValidationResultIdentifier::movies/20221122T054743.979136Z/20221122T054743.979136Z/2c7d67632e907942c3fcebd7c48acdac\n",
      "2022-11-22 11:17:50,533  INFO     [dq_ge:160]  Good Records Count: None\n",
      "2022-11-22 11:17:50,600  INFO     [dq_ge:181]  Bad Records Count: None\n",
      "2022-11-22 11:17:50,608  INFO     [dq_ge:406]  File C:/Users/abhinav.anand/Desktop/dq_check/result/result_2022-11-22-11-17-36.json Created in the Directory\n",
      "2022-11-22 11:18:19,626  INFO     [dq_ge:408]  DONE!!!\n",
      "2022-11-22 11:18:19,631  INFO     [dq_ge:4]  Process Completed Successfully!!!\n"
     ]
    }
   ],
   "source": [
    "dq_obj = GeDQFrameWork()\n",
    "try:\n",
    "    dq_obj.actual_process(\"movies\")\n",
    "    logger.info(\"Process Completed Successfully!!!\")\n",
    "except Exception as e:\n",
    "    logger.error(str(e)+\"Error Occured in the actual process\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "78e2df4fe7e8030381d9eb3a0cdd9f95f546cd90ef84515903623a27006596e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
